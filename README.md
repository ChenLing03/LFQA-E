# LFQA-E: Carefully Benchmarking Long-form QA Evaluation

## Overview

Long-form Question Answering (LFQA) involves generating detailed, paragraph-level responses to open-ended questions, which poses significant challenges for automatic evaluation. The LFQA-E benchmark is a comprehensive, multilingual, reference-based dataset designed to rigorously assess the performance of automatic evaluation metrics for LFQA.

LFQA-E includes:
- **1625 questions**
- **7649 comparisons**
- **15 diverse topics** such as engineering, law, biology, etc.

This benchmark provides expert-annotated reference answers, ensuring a more accurate and reliable evaluation process.

## Key Features
- **Multilingual**: LFQA-E contains questions and responses in both English and Chinese.
- **Diverse Topics**: It spans a variety of subjects, including natural sciences, social sciences, and technology.
- **Expert-annotated references**: Each question includes authorized reference answers validated by domain experts to provide a reliable baseline for evaluation.
- **Nuanced comparisons**: The dataset includes comparisons of responses that are difficult to differentiate, making it a challenging benchmark for automatic evaluation metrics.

## Contributions

- **Benchmark Construction**: LFQA-E fills the gap in LFQA evaluation by providing a well-constructed, multilingual, and reference-based dataset.
- **Evaluation Metrics Testing**: The benchmark is used to test various evaluation metrics, revealing that no current metric aligns with human judgments, especially for nuanced long-form responses.
- **Generalization**: The results highlight the struggles of current evaluation metrics in generalizing across topics and languages, especially when distinguishing between human and machine-generated responses.

## Dataset Overview

LFQA-E consists of two parts, both of them can be found in `data`:
1. **LFQA-E-EN (English version)**
2. **LFQA-E-ZH (Chinese version)**

The dataset includes:
- **Questions**: Detailed open-ended questions.
- **Responses**: Multiple responses to each question, including human-written answers and model generated answers.
- **References**: Expert-validated reference answers.

### Dataset Statistics
| Metric               | LFQA-E-EN  | LFQA-E-ZH  |
|----------------------|------------|------------|
| Topics               | 9          | 6          |
| Questions            | 1026       | 599        |
| Comparisons          | 6156       | 1493       |
| Avg Question Length  | 13.4       | 24.6       |
| Avg Reference Length | 299.1      | 187.2      |
| Avg Response Length  | 245.0      | 308.3      |

## Evaluation Metrics Tested

The LFQA-E benchmark can be used to evaluate a range of automatic evaluation metrics, including:
- **ROUGE**
- **BERTScore**
- **LLMs (Large Language Models)**
- **LRMs (Large Reasoning Models)**
- **RMs (Reward Models)**

## Methodology

LFQA-E evaluates the ability of different metrics to differentiate between two nuanced responses. The evaluation is performed using three settings:
1. **Human vs Human **
2. **Human vs Model **
3. **Model vs Model **

### Data Collection Process
- **Sources**: Data was sourced from online platforms like Reddit/ELI5 for English and academic examination papers for Chinese.
- **Expert Annotations**: Each response was annotated by experts to ensure the quality and reliability of the reference answers.
- **Model Response Generation**: Responses were generated by state-of-the-art LLMs.

## Results

The evaluation results highlight several key observations:
- **No metric consistently matches human performance**: While some metrics performed better than others, none were able to consistently match human judgments.
- **Challenges with nuanced comparisons**: Metrics struggled with distinguishing responses that were nearly identical in quality, especially in the model-to-model comparison.
- **Cross-language performance**: Evaluation metrics exhibited difficulties in generalizing across languages, with performance dropping when applied to the Chinese version of the dataset.

## Future Work

- **Expansion of Topics and Languages**: While the current version covers several domains and languages, expanding the topics and adding more languages would enhance the benchmark's applicability.
- **Improvement of Evaluation Metrics**: Future work should focus on developing more robust evaluation metrics that can handle the complexities of long-form answers, especially those that capture nuances and factual accuracy.

## License

LFQA-E is released under the [MIT License](LICENSE).

## Citation

If you use LFQA-E in your research, please cite the following paper:

